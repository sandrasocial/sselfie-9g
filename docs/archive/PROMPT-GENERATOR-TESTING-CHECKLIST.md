# Prompt Generator Testing & Validation Checklist

## ğŸ¯ Overview

This document provides a comprehensive testing checklist for the Prompt Generator system integrated into Maya's Studio Pro workbench mode.

---

## ğŸ“‹ Functionality Tests

### 1. Workbench Image Analysis
- [ ] **Test:** Workbench images are correctly analyzed
  - [ ] Upload 1 user LoRA image â†’ Analysis detects `containsPerson: true`
  - [ ] Upload 1 product image â†’ Analysis detects `containsProduct: true`
  - [ ] Upload 2 images (user + product) â†’ Both detected correctly
  - [ ] Upload 4 images â†’ All analyzed and positions tracked
  - [ ] Upload inspiration image â†’ Analysis detects style/mood
  - [ ] Empty workbench â†’ Handles gracefully (no errors)

### 2. Content Type Detection
- [ ] **Test:** Content type detection works for all scenarios
  - [ ] User says "create carousel" â†’ Detects `carousel_cover`, `carousel_content`
  - [ ] User says "educational infographic" â†’ Detects `carousel_infographic`
  - [ ] User says "morning routine UGC" â†’ Detects `ugc_morning_routine`
  - [ ] User says "coffee shop work" â†’ Detects `ugc_coffee_shop`
  - [ ] User says "product unboxing" â†’ Detects `ugc_unboxing`
  - [ ] User says "brand partnership skincare" â†’ Detects `brand_skincare`
  - [ ] User says "fashion collaboration" â†’ Detects `brand_fashion`
  - [ ] User says "tech product" â†’ Detects `brand_tech`
  - [ ] User says "reel cover tutorial" â†’ Detects `reel_tutorial`
  - [ ] User says "transformation reel" â†’ Detects `reel_transformation`
  - [ ] User says "day in the life" â†’ Detects `reel_lifestyle`
  - [ ] User says "educational reel" â†’ Detects `reel_educational`
  - [ ] Ambiguous intent â†’ Falls back to image-based inference

### 3. Template Selection
- [ ] **Test:** Templates are selected appropriately
  - [ ] Carousel content type â†’ Selects `CAROUSEL_COVER_SLIDE`, `CAROUSEL_CONTENT_SLIDE`
  - [ ] UGC content type â†’ Selects appropriate UGC template
  - [ ] Product content type â†’ Selects `PRODUCT_LIFESTYLE_MOCKUP` or `PRODUCT_FLAT_LAY`
  - [ ] Brand partnership â†’ Selects brand-specific template
  - [ ] Reel cover â†’ Selects reel cover template
  - [ ] Multiple content types â†’ Selects multiple templates
  - [ ] No matching templates â†’ Handles gracefully

### 4. Prompt Generation
- [ ] **Test:** Generated prompts include all required components
  - [ ] Character consistency instructions present (when user image exists)
  - [ ] Detailed subject description included
  - [ ] Clear action/pose specified
  - [ ] Specific environment described
  - [ ] Composition details (aspect ratio, shot type, framing)
  - [ ] Visual style and mood specified
  - [ ] Lighting setup described
  - [ ] Technical specs (lens, aperture, resolution)
  - [ ] Text space reserved (when needed)
  - [ ] Final use case specified

### 5. Character Consistency
- [ ] **Test:** Prompts maintain character consistency instructions
  - [ ] User image present â†’ Prompt includes "Keep facial features EXACTLY identical to Image 1"
  - [ ] Multiple slides â†’ Each slide references same character
  - [ ] Carousel series â†’ Consistency maintained across all slides
  - [ ] No user image â†’ No character consistency instructions (correct)

### 6. Capability Detection
- [ ] **Test:** NanoBanana Pro capabilities are correctly identified
  - [ ] Text overlay in prompt â†’ `text_rendering` capability detected
  - [ ] Multiple images referenced â†’ `multi_image_composition` detected
  - [ ] Character consistency mentioned â†’ `character_consistency` detected
  - [ ] Google Search mentioned â†’ `real_time_data` detected
  - [ ] Technical specs (lens, aperture) â†’ `professional_controls` detected
  - [ ] Infographic/educational â†’ `educational_excellence` detected
  - [ ] Multiple capabilities â†’ All detected correctly

### 7. Suggestion Ranking
- [ ] **Test:** Suggestions are ranked sensibly
  - [ ] Highest confidence suggestions appear first
  - [ ] Main variations ranked before alternate variations
  - [ ] More capabilities = higher rank (when confidence equal)
  - [ ] Ranking is consistent across multiple requests

### 8. Copy to Clipboard
- [ ] **Test:** Copy to clipboard works
  - [ ] Click "COPY" button â†’ Prompt copied to clipboard
  - [ ] Copied prompt matches suggestion exactly
  - [ ] Visual feedback shows "COPIED" state
  - [ ] Feedback disappears after 2 seconds
  - [ ] Works on mobile devices

### 9. Use in Workbench
- [ ] **Test:** Use in Workbench auto-fills prompt box
  - [ ] Click "USE IN WORKBENCH" â†’ Prompt appears in workbench prompt box
  - [ ] Workbench auto-expands if collapsed
  - [ ] Custom event dispatched correctly
  - [ ] Prompt box updates immediately
  - [ ] Works when workbench is already expanded
  - [ ] Works when workbench is collapsed

---

## âœ¨ Prompt Quality Tests

### 10. Carousel Prompts
- [ ] **Test:** Carousel prompts maintain consistency across slides
  - [ ] Slide 1 (cover) â†’ Includes hook and text space
  - [ ] Slide 2-4 (content) â†’ References "same person", "consistent styling"
  - [ ] Slide 5 (CTA) â†’ Maintains color palette from cover
  - [ ] All slides â†’ Same technical specs (lens, aperture)
  - [ ] All slides â†’ Same visual style mentioned
  - [ ] Text overlays â†’ Sophisticated typography (not generic)

### 11. UGC Prompts
- [ ] **Test:** UGC prompts include authenticity markers
  - [ ] Morning routine â†’ Includes "iPhone selfie aesthetic"
  - [ ] Coffee shop â†’ Includes "authentic imperfections"
  - [ ] Unboxing â†’ Includes "natural home setting"
  - [ ] All UGC â†’ Includes "realistic", "not overly staged"
  - [ ] All UGC â†’ Includes camera quality details

### 12. Product Mockup Prompts
- [ ] **Test:** Product mockup prompts integrate products naturally
  - [ ] Lifestyle mockup â†’ Product "naturally integrated, not forced"
  - [ ] Flat lay â†’ Product as focal point with supporting props
  - [ ] On-person â†’ Product placement clearly specified
  - [ ] All product prompts â†’ Product details accurately described
  - [ ] All product prompts â†’ Brand aesthetic matching mentioned

### 13. Brand Partnership Prompts
- [ ] **Test:** Brand partnership prompts match brand aesthetics
  - [ ] Skincare â†’ "Clean beauty, minimalist luxury" aesthetic
  - [ ] Fashion â†’ "Editorial street style" aesthetic
  - [ ] Tech â†’ "Premium lifestyle" aesthetic
  - [ ] All brand prompts â†’ Color palette extracted from references
  - [ ] All brand prompts â†’ Brand alignment mentioned

### 14. Reel Cover Prompts
- [ ] **Test:** Reel cover prompts optimize for engagement
  - [ ] Educational â†’ "Engaging expression", "direct eye contact"
  - [ ] Transformation â†’ "Clear visual payoff"
  - [ ] Lifestyle â†’ "Relatable aspiration" mood
  - [ ] Tutorial â†’ "Action frozen at most interesting moment"
  - [ ] All reel covers â†’ Text space for thumbnail visibility
  - [ ] All reel covers â†’ 9:16 vertical format specified

### 15. Technical Details
- [ ] **Test:** All prompts specify technical details
  - [ ] Lens specified (85mm, 50mm, etc.)
  - [ ] Aperture specified (f/2.0, f/2.8, etc.)
  - [ ] Resolution specified (2K, 4K)
  - [ ] Lighting direction and quality specified
  - [ ] Camera angle specified when relevant

### 16. Text Space
- [ ] **Test:** Text space is reserved when needed
  - [ ] Carousel cover â†’ "Top 30% reserved for headline"
  - [ ] Reel cover â†’ "Top 25% reserved for title"
  - [ ] Story graphic â†’ "Upper third with breathing room"
  - [ ] Infographic â†’ Text placement clearly specified
  - [ ] Non-text content â†’ No text space mentioned (correct)

### 17. Prompt Length
- [ ] **Test:** Prompts are under 500 words (NanoBanana Pro limit)
  - [ ] All generated prompts â†’ Under 500 words
  - [ ] Average prompt length â†’ 100-300 words (optimal)
  - [ ] Longest prompt â†’ Still under 500 words
  - [ ] Prompt length validation â†’ Works correctly

---

## ğŸ¨ User Experience Tests

### 18. Performance
- [ ] **Test:** Suggestions appear within 2 seconds
  - [ ] API response time â†’ Under 2 seconds
  - [ ] Loading state â†’ Shows skeleton while generating
  - [ ] Error handling â†’ Shows friendly error message
  - [ ] Network timeout â†’ Handles gracefully

### 19. Mobile Responsiveness
- [ ] **Test:** UI is responsive on mobile
  - [ ] Cards display correctly on mobile (< 375px width)
  - [ ] Buttons are touch-friendly (min 44px height)
  - [ ] Text is readable without zooming
  - [ ] Cards stack vertically on mobile
  - [ ] No horizontal scrolling required

### 20. Visual Design
- [ ] **Test:** Cards are visually appealing and clear
  - [ ] Matches SSELFIE design system (stone palette)
  - [ ] No emojis or decorative icons
  - [ ] Clean borders and spacing
  - [ ] Typography is clear and readable
  - [ ] Hover states work correctly
  - [ ] Loading skeletons match card layout

### 21. Capability Badges
- [ ] **Test:** Capability badges are informative
  - [ ] Badges show correct capability names
  - [ ] Badges are visually distinct
  - [ ] Multiple badges display correctly
  - [ ] Badges help users understand prompt features
  - [ ] No badges shown when no capabilities detected (correct)

### 22. User Understanding
- [ ] **Test:** Users understand which prompt to choose
  - [ ] Prompt names are descriptive
  - [ ] Descriptions explain what each prompt creates
  - [ ] Confidence scores help users choose
  - [ ] Use cases listed help users decide
  - [ ] Preview text shows enough context

### 23. Action Feedback
- [ ] **Test:** Copy/Use actions provide clear feedback
  - [ ] Copy button â†’ Shows "COPIED" with check icon
  - [ ] Use in Workbench â†’ Workbench expands and prompt appears
  - [ ] Visual feedback is immediate
  - [ ] Feedback is clear and noticeable
  - [ ] No confusion about what happened

---

## ğŸ“Š Success Metrics

### Technical Metrics

#### Prompt Generation Latency
- [ ] **Target:** < 2 seconds
- [ ] **Measurement:** Time from API call to suggestions displayed
- [ ] **Test:** Measure 10 requests, average should be < 2s
- [ ] **P95:** 95% of requests complete in < 2.5s

#### Template Matching Accuracy
- [ ] **Target:** > 90%
- [ ] **Measurement:** % of cases where correct template is selected
- [ ] **Test:** 20 test scenarios, 18+ should match correctly
- [ ] **Edge cases:** Ambiguous intents handled gracefully

#### User Selection Rate
- [ ] **Target:** > 70% users select suggestion vs. writing own
- [ ] **Measurement:** % of generations using suggested prompts
- [ ] **Test:** Track over 100 generations
- [ ] **Baseline:** Compare to manual prompt writing

### Quality Metrics

#### Generated Images Meet User Intent
- [ ] **Target:** > 85%
- [ ] **Measurement:** User confirms image matches what they wanted
- [ ] **Test:** Survey users after generation
- [ ] **Follow-up:** Track improvement over time

#### First-Try Success Rate
- [ ] **Target:** > 75%
- [ ] **Measurement:** % of users satisfied with first generated image
- [ ] **Test:** Track user satisfaction on first generation
- [ ] **Improvement:** Iterate on prompt quality based on feedback

#### User Satisfaction
- [ ] **Target:** > 4.5/5
- [ ] **Measurement:** User rating of prompt suggestions
- [ ] **Test:** In-app survey after using suggestions
- [ ] **Feedback:** Collect qualitative feedback

### Engagement Metrics

#### Multiple Suggestions Tried
- [ ] **Target:** Users try 2+ suggestions per session
- [ ] **Measurement:** Average suggestions used per user
- [ ] **Test:** Track usage over 1 week
- [ ] **Goal:** Users explore variations

#### Variations Explored
- [ ] **Target:** Users try different variations
- [ ] **Measurement:** % of users who try multiple variations
- [ ] **Test:** Track variation selection
- [ ] **Insight:** Which variations are most popular?

#### Learning Curve
- [ ] **Target:** Time to first successful generation < 5 minutes
- [ ] **Measurement:** Time from first suggestion to successful generation
- [ ] **Test:** Track new user onboarding
- [ ] **Improvement:** Optimize onboarding flow

---

## ğŸ§ª Test Scenarios

### Scenario 1: New User - First Carousel
1. User opens workbench
2. Uploads 1 user photo
3. Asks: "Create a carousel about morning routines"
4. **Expected:** 3 suggestions for carousel cover/content
5. **Verify:** All include character consistency, text space, technical details

### Scenario 2: Brand Partnership
1. User uploads user photo + product image
2. Asks: "Create brand content with this skincare product"
3. **Expected:** Suggestions for skincare brand partnership
4. **Verify:** Product integrated naturally, brand aesthetic matched

### Scenario 3: Reel Cover with Text
1. User uploads user photo
2. Asks: "Create reel cover saying '5 Productivity Tips'"
3. **Expected:** Reel cover suggestions with sophisticated text overlay
4. **Verify:** Text broken into parts, Instagram fonts specified

### Scenario 4: Educational Infographic
1. User uploads style reference
2. Asks: "Create infographic about Instagram algorithm"
3. **Expected:** Infographic template with real-time data capability
4. **Verify:** Text rendering capability detected, layout specified

### Scenario 5: Multiple Variations
1. User uploads user photo
2. Asks: "Create lifestyle content"
3. **Expected:** Multiple template variations (UGC, lifestyle, etc.)
4. **Verify:** Variations ranked by confidence, all valid

---

## ğŸ› Known Issues & Edge Cases

### Edge Cases to Test
- [ ] Empty workbench (no images)
- [ ] Maximum images (4 images)
- [ ] Very long user intent (> 500 characters)
- [ ] Ambiguous intent ("create something")
- [ ] Intent with typos
- [ ] Multiple content types in one request
- [ ] Rapid successive requests
- [ ] Network failure during generation
- [ ] API timeout
- [ ] Invalid image URLs

### Error Handling
- [ ] API errors â†’ User-friendly error message
- [ ] Network errors â†’ Retry option or clear error
- [ ] Invalid responses â†’ Graceful degradation
- [ ] Missing data â†’ Defaults applied correctly

---

## âœ… Sign-Off Checklist

### Development Team
- [ ] All functionality tests passed
- [ ] All prompt quality tests passed
- [ ] All UX tests passed
- [ ] Performance targets met
- [ ] Error handling verified

### QA Team
- [ ] Full regression test completed
- [ ] Edge cases tested
- [ ] Mobile testing completed
- [ ] Cross-browser testing completed

### Product Team
- [ ] Success metrics baseline established
- [ ] User testing completed
- [ ] Feedback incorporated
- [ ] Documentation updated

---

## ğŸ“ Notes

- **Test Environment:** Use staging environment with test data
- **Test Users:** Create test accounts with various image types
- **Monitoring:** Set up analytics to track metrics automatically
- **Feedback Loop:** Collect user feedback continuously
- **Iteration:** Update prompts based on generation results

---

**Last Updated:** [Date]
**Version:** 1.0
**Status:** Ready for Testing



